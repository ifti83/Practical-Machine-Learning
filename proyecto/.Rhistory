options("viewer"=NULL)
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# usage
packages <- c("devtools", "knitr", "xlsx")
ipak(packages)
install_github('ramnathv/rCharts')
library(swirl)
swirl()
plot(child ~ parent, galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
fit <- lm(child ~ parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs-rhs
all.equal(lhs,rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
attenu$mag
cov(efit$residuals, attenu$mag)
cov(attenu$mag,efit$residuals)
cov(efit$residuals, attenu$dist)
cor(gpa_nor,gch_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
fit <- lm(child ~ parent, galton)
sqrt(sum(fit$residuals^2) / (n - 2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
1-sRes/sTot
summary(fit)$r.squared
cor(galton$parent,galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent - 1, galton)
lm(child ~ parent, galton)
lm(child ~ 1, galton)
head(trees)
fit <- lm(Volume ~ . - 1, trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ . - 1, trees2)
lapply(list(fit, fit2), coef)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(slidify)
install.packages("Hmisc")
data(mtcars)
data(mtcars)
require(GGally)
install.pakcages(GGally)
install.packages(GGally)
install.packages("GGally")
```{r}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
library(randomForest)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
library(randomForest)
library(varImp)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
c3
setwd("/Users/pacha/Desktop/base de datos exportadores")library(foreign)
setwd("/Users/pacha/Desktop/base de datos exportadores")library(foreign)
#REEMPLAZAR LAS " SI SE PEGA DESDE EXCELsetwd("/Users/pacha/Desktop/base de datos exportadores")library(foreign)CIUDADES <- read.dbf("CIUDADES.DBF")DIRPAI <- read.dbf("DIRPAI.DBF")EJER_AUX <- read.dbf("EJER_AUX.DBF")EJETEMP <- read.dbf("EJETEMP.DBF")EXPODIRA <- read.dbf("EXPODIRA.DBF")EXPOENCA <- read.dbf("EXPOENCA.DBF")EXPOFONO <- read.dbf("EXPOFONO.DBF")EXPOMAE <- read.dbf("EXPOMAE.DBF")EXPONAB <- read.dbf("EXPONAB.DBF")NEWNAB <- read.dbf("NEWNAB.DBF")RUB_AUX <- read.dbf("RUB_AUX.DBF")SALIDA <- read.dbf("SALIDA.DBF")TABPAI <- read.dbf("TABPAI.DBF")TEMPAI <- read.dbf("TEMPAI.DBF")TEMPORAL <- read.dbf("TEMPORAL.DBF")write.csv(CIUDADES, file = "CIUDADES.csv")write.csv(DIRPAI, file = "DIRPAI.csv")write.csv(EJER_AUX, file = "EJER_AUX.csv")write.csv(EJETEMP, file = "EJETEMP.csv")write.csv(EXPODIRA, file = "EXPODIRA.csv")write.csv(EXPOENCA, file = "EXPOENCA.csv")write.csv(EXPOFONO, file = "EXPOFONO.csv")write.csv(EXPOMAE, file = "EXPOMAE.csv")write.csv(EXPONAB, file = "EXPONAB.csv")write.csv(NEWNAB, file = "NEWNAB.csv")write.csv(RUB_AUX, file = "RUB_AUX.csv")write.csv(SALIDA, file = "SALIDA.csv")write.csv(TABPAI, file = "TABPAI.csv")write.csv(TEMPAI, file = "TEMPAI.csv")write.csv(TEMPORAL, file = "TEMPORAL.csv")
#REEMPLAZAR LAS " SI SE PEGA DESDE EXCELsetwd("/Users/pacha/Desktop/base de datos exportadores")library(foreign)CIUDADES <- read.dbf("CIUDADES.DBF")DIRPAI <- read.dbf("DIRPAI.DBF")EJER_AUX <- read.dbf("EJER_AUX.DBF")EJETEMP <- read.dbf("EJETEMP.DBF")EXPODIRA <- read.dbf("EXPODIRA.DBF")EXPOENCA <- read.dbf("EXPOENCA.DBF")EXPOFONO <- read.dbf("EXPOFONO.DBF")EXPOMAE <- read.dbf("EXPOMAE.DBF")EXPONAB <- read.dbf("EXPONAB.DBF")NEWNAB <- read.dbf("NEWNAB.DBF")RUB_AUX <- read.dbf("RUB_AUX.DBF")SALIDA <- read.dbf("SALIDA.DBF")TABPAI <- read.dbf("TABPAI.DBF")TEMPAI <- read.dbf("TEMPAI.DBF")TEMPORAL <- read.dbf("TEMPORAL.DBF")write.csv(CIUDADES, file = "CIUDADES.csv")write.csv(DIRPAI, file = "DIRPAI.csv")write.csv(EJER_AUX, file = "EJER_AUX.csv")write.csv(EJETEMP, file = "EJETEMP.csv")write.csv(EXPODIRA, file = "EXPODIRA.csv")write.csv(EXPOENCA, file = "EXPOENCA.csv")write.csv(EXPOFONO, file = "EXPOFONO.csv")write.csv(EXPOMAE, file = "EXPOMAE.csv")write.csv(EXPONAB, file = "EXPONAB.csv")write.csv(NEWNAB, file = "NEWNAB.csv")write.csv(RUB_AUX, file = "RUB_AUX.csv")write.csv(SALIDA, file = "SALIDA.csv")write.csv(TABPAI, file = "TABPAI.csv")write.csv(TEMPAI, file = "TEMPAI.csv")write.csv(TEMPORAL, file = "TEMPORAL.csv")
library(foreign)
help(foreign)
??foreign
setwd("/Users/pacha/Desktop/base de datos exportadores")library(foreign)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f <- lm(y ~ x)
summary(f) # p-value: 0.05296
data(mtcars)
fit <- lm(mpg ~ I(wt - mean(wt)), data = mtcars)
confint(fit)
fit <- lm(mpg ~ wt, data = mtcars)
predict(fit, newdata = data.frame(wt = 3), interval = "prediction")
fit <- lm(mpg ~ wt, data = mtcars)
confint(fit)[2, ] * 2
fit1 <- lm(mpg ~ wt, data = mtcars)
fit2 <- lm(mpg ~ 1, data = mtcars)
1 - summary(fit1)$r.squared
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5]
fit <- lm(y ~ x)
lm.influence(fit)$hat[5]
hatvalues(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
hatvalues(fit) #0.9945
dfbetas(fit)
library(MASS)
data(shuttle)
## Make our own variables just for illustration
shuttle$auto <- 1 * (shuttle$use == "auto")
shuttle$headwind <- 1 * (shuttle$wind == "head")
fit <- glm(auto ~ headwind, data = shuttle, family = binomial)
exp(coef(fit))
shuttle$auto <- 1 * (shuttle$use == "auto")
shuttle$headwind <- 1 * (shuttle$wind == "head")
fit <- glm(auto ~ headwind + magn, data = shuttle, family = binomial)
exp(coef(fit))
fit <- glm(count ~ relevel(spray, "B"), data = InsectSprays, family = poisson)
exp(coef(fit))[2]
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
z <- (x > 0) * x
fit <- lm(y ~ x + z)
sum(coef(fit)[2:3])
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit)
#Reading and Cleaning Data
#Read both training and testing instances. I have used a function named LOAD to load the packages that I will use later.
setwd("/Users/pacha/Documents/Coursera/tareas y controles/Practical Machine Learning/proyecto")
load <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
training_data <- read.csv("pml-training.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
testing_data <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
#I need to drop columns with NAs, drop highly correlated variables and drop variables with 0 (or approx to 0) variance.
str(training)
cleantraining <- training_data[, -which(names(training_data) %in% c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"))]
cleantraining = cleantraining[, colSums(is.na(cleantraining)) == 0] #this drops columns with NAs
zerovariance =nearZeroVar(cleantraining[sapply(cleantraining, is.numeric)], saveMetrics=TRUE)
cleantraining = cleantraining[, zerovariance[, 'nzv'] == 0] #to remove 0 or near to 0 variance variables
correlationmatrix <- cor(na.omit(cleantraining[sapply(cleantraining, is.numeric)]))
dim(correlationmatrix)
correlationmatrixdegreesoffreedom <- expand.grid(row = 1:52, col = 1:52)
correlationmatrixdegreesoffreedom$correlation <- as.vector(correlationmatrix) #this returns the correlation matrix in matrix format
removehighcorrelation <- findCorrelation(correlationmatrix, cutoff = .7, verbose = TRUE)
cleantraining <- cleantraining[, -removehighcorrelation] #this removes highly correlated variables (in psychometric theory .7+ correlation is a high correlation)
for(i in c(8:ncol(cleantraining)-1)) {cleantraining[,i] = as.numeric(as.character(cleantraining[,i]))}
for(i in c(8:ncol(testing_data)-1)) {testing_data[,i] = as.numeric(as.character(testing_data[,i]))} #Some columns were blank, hence are dropped. I will use a set that only includes complete columns. I also remove user name, timestamps and windows to have a light data set.
featureset <- colnames(cleantraining[colSums(is.na(cleantraining)) == 0])[-(1:7)]
modeldata <- cleantraining[featureset]
featureset #now we have the model data built from our feature set.
##Cross-Validation
#I need to split the sample in two samples. This is to divide training and testing for cross-validation.
idx <- createDataPartition(modeldata$classe, p=0.6, list=FALSE )
training <- modeldata[idx,]
testing <- modeldata[-idx,]
control <- trainControl(method="cv", 5)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
model
predict <- predict(model, testing)
confusionMatrix(testing$classe, predict)
accuracy <- postResample(predict, testing$classe)
accuracy
result <- predict(model, training[, -length(names(training))])
result
treeModel <- rpart(classe ~ ., data=cleantraining, method="class")
prp(treeModel)
##############################
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
testing_data <- testing_data[featureset[featureset!='classe']]
answers <- predict(model, newdata=testing_data)
answers
pml_write_files(answers)
